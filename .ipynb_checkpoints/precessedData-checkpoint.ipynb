{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "import re\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "stopwords = STOPWORDS\n",
    "\n",
    "\n",
    "# fquestion = 'fields.html'\n",
    "# fanswer = 'evaluator13709346/evaluation2802.html'\n",
    "# fanswer = 'evaluator1904283/evaluation189.html'\n",
    "# fanswer = 'evaluator6895718/evaluation1714.html'\n",
    "# fanswer = 'evaluator8542011/evaluation683.html'\n",
    "# questionlen = 0\n",
    "# user_id = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processedQuestion(q):\n",
    "    tmp = []\n",
    "    string = q.get_text()\n",
    "    que = string[:string.find(':')+1]\n",
    "    #去电多余的空格，只留一个\n",
    "    que = ' '.join(que.split())\n",
    "    \n",
    "    tmp.append(que)\n",
    "    lis = q.find_all('li')\n",
    "    for li in lis:\n",
    "#         print(str(li)[str(li).find('<li>')+4:str(li).find('.')])\n",
    "        a = str(li)[str(li).find('<li>')+4:str(li).find('.')]\n",
    "        a = ' '.join(a.split())\n",
    "        tmp.append(a)\n",
    "\n",
    "    return tmp\n",
    "\n",
    "def findQuestion(texts):\n",
    "    '''\n",
    "    找到问题1\\2\\3\\4的位置\n",
    "    问题下面的div对一个的是答案\n",
    "    '''\n",
    "    return texts.get_text().find('Q1')>=0 or texts.get_text().find('Q2')>=0 or texts.get_text().find('Q3')>=0 or texts.get_text().find('Q4')>=0\n",
    "    \n",
    "\n",
    "#格式化输入的试卷文件\n",
    "def processedQuestionFile(filepath):\n",
    "    f = open(filepath, 'rb')\n",
    "    htmlstring = f.read()\n",
    "    #得到所有的div块的文件\n",
    "    soup = BeautifulSoup(htmlstring)\n",
    "    \n",
    "    user_id = soup.title.get_text()\n",
    "#     print(soup.title)\n",
    "    \n",
    "    texts = soup.find_all('div')\n",
    "#     print(\"len(texts):\",len(texts))\n",
    "#     print(texts)\n",
    "    \n",
    "    Questions_and_Answers = [] #结构 [大问题，子问题，子问题,答案],[大问题，子问题，子问题，答案]...,[大问题，子问题，子问题，答案]\n",
    "    for i in range(0,len(texts)-1,1):\n",
    "        if findQuestion(texts[i]): \n",
    "#             print(texts[i])\n",
    "            tmp = processedQuestion(texts[i])\n",
    "#             print(tmp)\n",
    "            a = texts[i+1].get_text()\n",
    "#             print(a)\n",
    "            a = ' '.join(a.split())\n",
    "            tmp.append(a)\n",
    "            Questions_and_Answers.append(tmp)\n",
    "\n",
    "    # 去掉/n的换行\n",
    "    for i in range(len(Questions_and_Answers)):\n",
    "        for j in range(len(Questions_and_Answers[i])):\n",
    "            try:\n",
    "                Questions_and_Answers[i][j] = Questions_and_Answers[i][j].replace('\\n',' ')\n",
    "            except Exception:\n",
    "                Questions_and_Answers[i][j] = []\n",
    "        \n",
    "    Questions_and_Answers_s = sorted(Questions_and_Answers)#排序，第一题在前面\n",
    "    \n",
    "    Questions_and_Answers = []\n",
    "    for i in range(len(Questions_and_Answers_s)):\n",
    "        if len(Questions_and_Answers_s[i][0]) >0 and Questions_and_Answers_s[i][0][0] == 'Q':\n",
    "            Questions_and_Answers.append(Questions_and_Answers_s[i])\n",
    "            \n",
    "    \n",
    "    \n",
    "    #得到小问题的个数\n",
    "    questionlen = 0\n",
    "    for i in range(len(Questions_and_Answers)):\n",
    "        questionlen += len(Questions_and_Answers[i]) - 2\n",
    "        \n",
    "    \n",
    "    return Questions_and_Answers,user_id,questionlen\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#格式化结果文件,[[问题，结果，评分],...,[问题，结果，评分]]\n",
    "def processedAnswerFile(filepath,questionlen):\n",
    "    f = open(filepath, 'rb')\n",
    "    htmlstring = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(htmlstring)\n",
    "    texts = soup.find_all('div') #得到所有的div块的数据\n",
    "    \n",
    "#     print(len(texts))\n",
    "    n = min(int(questionlen * 2),34)\n",
    "    evalution = []\n",
    "    #去掉最后一个div，最后一个不是评论的答案\n",
    "    for i in range(0, n-1,2):\n",
    "        t = texts[i].get_text()\n",
    "        question = t[:t.find(':')]\n",
    "        answer = t[t.find(':')+1:].replace(\"\\n\",\" \")\n",
    "        \n",
    "        question = ' '.join(question.split())\n",
    "        answer = ' '.join(answer.split())\n",
    "        \n",
    "#         print(texts[i+1].get_text())\n",
    "        \n",
    "        evalution.append([question,answer,int(texts[i+1].get_text())])\n",
    "\n",
    "    #去掉特殊字符'\\xa0'\n",
    "    for i in range(len(evalution)):\n",
    "        evalution[i][0] = evalution[i][0].replace(\"\\xa0\",\"\")\n",
    "        evalution[i][1] = evalution[i][1].replace(\"\\xa0\",\"\")\n",
    "    \n",
    "    return evalution\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countIDF(text,topK = 12):\n",
    "    '''\n",
    "    text:字符串，topK根据TF-IDF得到前topk个关键词的词频，用于计算相似度\n",
    "    return 词频vector\n",
    "    '''\n",
    "    from jieba import analyse\n",
    "    tfidf = analyse.extract_tags\n",
    "\n",
    "    cipin = {} #统计分词后的词频\n",
    "    text = text[:text.find('.')]\n",
    "    fencis = text.split(\" \")\n",
    "    \n",
    "    #处理特殊字符\n",
    "    fenci = []\n",
    "    for i in range(len(fencis)):\n",
    "        fenci.extend(fencis[i].split(\"-\"))\n",
    "    \n",
    "    fencis = fenci\n",
    "    fenci = []\n",
    "    for i in range(len(fencis)):\n",
    "        fenci.extend(fencis[i].split(\"/\"))\n",
    "    \n",
    "#     print(fenci)\n",
    "    #记录每个词频的频率\n",
    "    for word in fenci:\n",
    "        if word not in cipin.keys():\n",
    "            cipin[word] = 0\n",
    "        cipin[word] += 1\n",
    "\n",
    "    # 基于tfidf算法抽取前10个关键词，包含每个词项的权重\n",
    "    keywords = tfidf(text,topK,withWeight=True)\n",
    "#     print(keywords)\n",
    "    ans = []\n",
    "    # keywords.count(keyword)得到keyword的词频\n",
    "    # help(tfidf)\n",
    "    # 输出抽取出的关键词\n",
    "    for keyword in keywords:\n",
    "        #print(keyword ,\" \",cipin[keyword[0]])\n",
    "        if keyword[0] in cipin.keys():\n",
    "            ans.append(cipin[keyword[0]]) #得到前topk频繁词项的词频的频率\n",
    "\n",
    "    return keywords, cipin\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    '''\n",
    "    a，b的余弦相似度\n",
    "    '''\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "        \n",
    "    #return {\"文本的余弦相似度:\":np.sum(a*b) / (np.sqrt(np.sum(a ** 2)) * np.sqrt(np.sum(b ** 2)))}\n",
    "    return np.sum(a*b) / (np.sqrt(np.sum(a ** 2)) * np.sqrt(np.sum(b ** 2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lcs(str_a, str_b):\n",
    "    lensum = float(len(str_a) + len(str_b))\n",
    "    #得到一个二维的数组，类似用dp[lena+1][lenb+1],并且初始化为0\n",
    "    lengths = [[0 for j in range(len(str_b)+1)] for i in range(len(str_a)+1)]\n",
    "\n",
    "    #enumerate(a)函数： 得到下标i和a[i]\n",
    "    for i, x in enumerate(str_a):\n",
    "        for j, y in enumerate(str_b):\n",
    "            if x == y:\n",
    "                lengths[i+1][j+1] = lengths[i][j] + 1\n",
    "            else:\n",
    "                lengths[i+1][j+1] = max(lengths[i+1][j], lengths[i][j+1])\n",
    "    \n",
    "    return lengths[len(str_a)][len(str_b)]\n",
    "    \n",
    "def equals(a,b):\n",
    "    '''\n",
    "    如果两个字符串的所有相同单词的个数并 == 单数数-2 ，\n",
    "    则认为是同一个句子的，不同表达，注意仅在次应用中有用\n",
    "    '''\n",
    "    return lcs(a,b) >= min(len(a),len(b)) - 2\n",
    "\n",
    "def getallAnswers(Questions_and_Answers,evalution):\n",
    "    '''\n",
    "    得到所有的答案，作为总的文本计算tfidf\n",
    "    包括评价文本，答案文本，\n",
    "    '''\n",
    "    allAnswers = []\n",
    "    \n",
    "    n = len(Questions_and_Answers)\n",
    "    #整合每一道提的答案和标准的评分答案\n",
    "    for j in range(n):\n",
    "        data = Questions_and_Answers[j]\n",
    "        Answers = []\n",
    "        Answers.append(data[-1])\n",
    "        for i in range(1,len(data)-1):\n",
    "#             print(data[i])\n",
    "            for k in range(len(evalution)):\n",
    "                if equals(data[i], evalution[k][0]):\n",
    "#                     print(evalution[k][0])\n",
    "                    Answers.append([evalution[k][1],evalution[k][2]])\n",
    "        \n",
    "        allAnswers.append(Answers)\n",
    "        \n",
    "    return allAnswers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSource(myAnswer, normalAnswer,source, topK = 12):\n",
    "    '''\n",
    "    myAnswser : 测试的答案\n",
    "    normalAnswer ：标准打分答案\n",
    "    topk ：频繁词项的个数\n",
    "    source : 评分\n",
    "    '''\n",
    "#     print(\"myA\",myAnswer)\n",
    "#     print(\"normal\",normalAnswer)\n",
    "\n",
    "    keyword0,cinpin0 = countIDF(myAnswer,topK)\n",
    "    keyword1,cinpin1 = countIDF(normalAnswer,topK)    \n",
    "#     print(\"cinpin0\",cinpin0)\n",
    "    \n",
    "    keywords = set()\n",
    "    for d in keyword0:\n",
    "        keywords.add(d[0])\n",
    "\n",
    "    for d in keyword1:\n",
    "        keywords.add(d[0])\n",
    "\n",
    "    # print(keywords)\n",
    "    tf0 = []\n",
    "    tf1 = []\n",
    "    for d in keywords:\n",
    "    #     print(d)\n",
    "        if d in cinpin0.keys():\n",
    "            tf0.append(cinpin0[d] / len(cinpin0))\n",
    "        else:\n",
    "            tf0.append(0)\n",
    "        if d in cinpin1.keys():\n",
    "            tf1.append(cinpin1[d] / len(cinpin1))\n",
    "        else:\n",
    "            tf1.append(0)\n",
    "#     print(tf0,tf1)\n",
    "#     print(cos_sim(tf0,tf1) * source)\n",
    "    return cos_sim(tf0,tf1) * source\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(text):\n",
    "    '''\n",
    "    处理文本\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    words = re.sub(\"\\W\",\" \",text).split()\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    return words\n",
    "\n",
    "# processed_docs = [tokenize(doc) for doc in documents]\n",
    "\n",
    "def similarityLDA(myAnswer, normalAnswer,source, topK = 12):\n",
    "    '''\n",
    "    根据LDA计算相似度，根据评分答案，给出相应的分数\n",
    "    根据回答问题的答案，得到LDA的topic模型，\n",
    "    得到评分答案的topic模型，然后在比较两个文本的相似度\n",
    "    '''\n",
    "    #回答问题的答案，得到LDA的topic模型\n",
    "    processed_docs = [tokenize(myAnswer)]\n",
    "    \n",
    "#     print(processed_docs)\n",
    "    dic = corpora.Dictionary(processed_docs) #构造词典  \n",
    "# --------------------------------------------------------------------------------------------    \n",
    "#     corpus = [dic.doc2bow(text) for text in processed_docs] # 每个text 对应的稀疏向量  \n",
    "#     tfidf = models.TfidfModel(corpus) #统计tfidf  \n",
    "#     corpus_tfidf = tfidf[corpus]  #得到每个文本的tfidf向量，稀疏矩阵  \n",
    "    \n",
    "#     lda = models.LdaModel(corpus_tfidf, id2word = dic, num_topics = topK)   \n",
    "    \n",
    "    lda = models.LdaModel(id2word = dic, num_topics = topK)   \n",
    "#----------------------------------------------------------------------------------------------\n",
    "#     corpus_lda = lda[corpus_tfidf]\n",
    "    \n",
    "    \n",
    "    # 打印前10个topic的词分布，这儿可以进行输出\n",
    "    lda.print_topics(10)\n",
    "    \n",
    "    \n",
    "    #计算normalAnswer的相似度\n",
    "    test_doc = tokenize(normalAnswer)#新文档进行分词\n",
    "    doc_bow = dic.doc2bow(test_doc)      #文档转换成bow\n",
    "    doc_lda = lda[doc_bow]                   #得到新文档的主题分布，\n",
    "    #输出新文档的主题分布\n",
    "    #print(doc_lda)\n",
    "    \n",
    "    #这里去第一个作为最相关的主题\n",
    "    topicid = doc_lda[0][0]\n",
    "    topicSim = doc_lda[0][1]\n",
    "    \n",
    "    return source * topicSim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Questions_and_Answers,user_id,questionlen = processedQuestionFile(fquestion)\n",
    "# evalution = processedAnswerFile(fanswer)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# allAnswers = getallAnswers(Questions_and_Answers,evalution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(Questions_and_Answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getOneSourceAll(Questions_and_Answers,evalution):\n",
    "    allAnswers = getallAnswers(Questions_and_Answers,evalution)\n",
    "#     print('allAnswers-------',allAnswers)\n",
    "    onesourceall = []\n",
    "    for i in range(len(allAnswers)):\n",
    "        sourceall = []\n",
    "        for j in range(1,len(allAnswers[i])):\n",
    "            #这儿可以选择两种模型进行计算相似度的得分\n",
    "#             sourceall.append(getSource(allAnswers[i][0],allAnswers[i][j][0],allAnswers[i][j][1]))\n",
    "            sourceall.append(similarityLDA(allAnswers[i][0],allAnswers[i][j][0],allAnswers[i][j][1]))\n",
    "            \n",
    "        onesourceall.append(sourceall)\n",
    "    return onesourceall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 得到每个答案的关键次\n",
    "def getQuestonKeys(filepath):\n",
    "    Questions_and_Answers,user_id,questionlen = processedQuestionFile(filepath)\n",
    "\n",
    "    QuestonKeys = []\n",
    "    topK = 12 #可以修改关键词的个数\n",
    "    for i in range(len(Questions_and_Answers)):\n",
    "        myAnswer = Questions_and_Answers[i][-1]\n",
    "        keyword0,cinpin0 = countIDF(myAnswer,topK)\n",
    "        \n",
    "        keywords = [keyword0[i][0] for i in range(len(keyword0))]\n",
    "        QuestonKeys.append(keywords)\n",
    "    \n",
    "    return QuestonKeys\n",
    "\n",
    "\n",
    "#得到每个问题答案的LDA的矩阵\n",
    "def getQuestonLDAMat(filepath):\n",
    "    Questions_and_Answers,user_id,questionlen = processedQuestionFile(filepath)\n",
    "    \n",
    "    QuestonLDAMat = []\n",
    "    for i in range(len(Questions_and_Answers)):\n",
    "        myAnswer = Questions_and_Answers[i][-1]\n",
    "        #回答问题的答案，得到LDA的topic模型\n",
    "        processed_docs = [tokenize(myAnswer)]\n",
    "        # print(processed_docs)\n",
    "        dic = corpora.Dictionary(processed_docs) #构造词典  \n",
    "# -------------------------------------------------------------------------------------------\n",
    "#         corpus = [dic.doc2bow(text) for text in processed_docs] # 每个text 对应的稀疏向量  \n",
    "#         tfidf = models.TfidfModel(corpus) #统计tfidf  \n",
    "#         corpus_tfidf = tfidf[corpus]  #得到每个文本的tfidf向量，稀疏矩阵  \n",
    "#---------------------------------------------------------------------------------------------\n",
    "        lda = models.LdaModel(id2word = dic, num_topics = 12)   \n",
    "    \n",
    "        QuestonLDAMat.append(lda.print_topics())\n",
    "    \n",
    "    return QuestonLDAMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# onesourceall = getOneSourceAll(Questions_and_Answers,evalution)\n",
    "\n",
    "# onesourceall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#集成一个文件进行测试：\n",
    "def uniteAll(filelist):\n",
    "    \n",
    "    if len(filelist)<2 :\n",
    "        return None,None\n",
    "    \n",
    "    fquestion = filelist[0]\n",
    "    sourcelist = []\n",
    "    \n",
    "    for i in range(1,len(filelist)):\n",
    "        fanswer = filelist[i]\n",
    "        Questions_and_Answers,user_id,questionlen = processedQuestionFile(fquestion)\n",
    "#         print(Questions_and_Answers,questionlen)\n",
    "        \n",
    "        evalution = processedAnswerFile(fanswer,questionlen)\n",
    "#         print(evalution)\n",
    "        user_id = user_id[user_id.find(':')+1:user_id.find(')')].strip()\n",
    "        onesourceall = getOneSourceAll(Questions_and_Answers,evalution)\n",
    "        \n",
    "#         print(onesourceall)\n",
    "        \n",
    "        sourcelist.append(onesourceall)\n",
    "#         print(onesourceall)\n",
    "    \n",
    "    source = []\n",
    "    try:\n",
    "        for k in range(min(len(sourcelist[0]),4)):\n",
    "            tmp = sourcelist[0][k]\n",
    "            for i in range(1,len(sourcelist)):\n",
    "                tmp =[tmp[j] + sourcelist[i][k][j] for j in range(len(sourcelist[i][k]))]\n",
    "\n",
    "            source.append(tmp)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    source4 = []\n",
    "    for i in range(len(source)):\n",
    "        source4.append(np.sum(np.array(source[i]))/len(source[i]))\n",
    "        \n",
    "    return user_id,source4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filelist = ['/home/xiaoran/tmpfiles/tmp/submission441/fields.html',\n",
    " '/home/xiaoran/tmpfiles/tmp/submission441/evaluator13709346/evaluation2802.html',\n",
    " '/home/xiaoran/tmpfiles/tmp/submission441/evaluator6895718/evaluation1714.html',\n",
    " '/home/xiaoran/tmpfiles/tmp/submission441/evaluator9905753/evaluation2022.html',\n",
    " '/home/xiaoran/tmpfiles/tmp/submission441/evaluator8542011/evaluation683.html',\n",
    " '/home/xiaoran/tmpfiles/tmp/submission441/evaluator1904283/evaluation189.html']\n",
    "\n",
    "\n",
    "filelist1 = ['1/assessment5/submitter4259923/submission388/fields.html',\n",
    " '1/assessment5/submitter4259923/submission388/evaluator9247166/evaluation245.html',\n",
    " '1/assessment5/submitter4259923/submission388/evaluator13826915/evaluation1029.html',\n",
    " '1/assessment5/submitter4259923/submission388/evaluator5456952/evaluation1752.html',\n",
    " '1/assessment5/submitter4259923/submission388/evaluator1755159/evaluation2330.html']\n",
    "\n",
    "filelist2 = ['2/assessment5/submitter7044176/submission660/fields.html',\n",
    " '2/assessment5/submitter7044176/submission660/evaluator7429191/evaluation468.html',\n",
    " '2/assessment5/submitter7044176/submission660/evaluator4272669/evaluation1835.html',\n",
    " '2/assessment5/submitter7044176/submission660/evaluator12076/evaluation4047.html',\n",
    " '2/assessment5/submitter7044176/submission660/evaluator5922011/evaluation3014.html']\n",
    "\n",
    "\n",
    "filelist3 = ['3/assessment5/submitter1642623/submission2125/fields.html']\n",
    "\n",
    "filelist4 = ['3/assessment5/submitter4414416/submission2057/fields.html',\n",
    " '3/assessment5/submitter4414416/submission2057/evaluator2235434/evaluation222.html',\n",
    " '3/assessment5/submitter4414416/submission2057/evaluator3704292/evaluation5822.html',\n",
    " '3/assessment5/submitter4414416/submission2057/evaluator2941173/evaluation7811.html',\n",
    " '3/assessment5/submitter4414416/submission2057/evaluator2382623/evaluation3408.html',\n",
    " '3/assessment5/submitter4414416/submission2057/evaluator1720038/evaluation9621.html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiaoran/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file /home/xiaoran/anaconda3/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "user_id,source = uniteAll(filelist1)\n",
    "\n",
    "keywords = getQuestonKeys(filelist1[0])\n",
    "\n",
    "LDAMat = getQuestonLDAMat(filelist1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('7dcc6fe812d4b78ddf00cbb02d540bce8defcd55',\n",
       " [1.2099883774851061,\n",
       "  6.6197971810996226,\n",
       "  1.0763745124119646,\n",
       "  0.52033850676656468])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id,source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['throughout',\n",
       "  'present',\n",
       "  'cell',\n",
       "  'intergenic',\n",
       "  'regions',\n",
       "  'largely',\n",
       "  'methylation',\n",
       "  'genome',\n",
       "  'spreaded',\n",
       "  'more',\n",
       "  'being',\n",
       "  'normal'],\n",
       " ['both',\n",
       "  'imprinting',\n",
       "  'loss',\n",
       "  'cells',\n",
       "  'likely',\n",
       "  'Cancer',\n",
       "  'specific',\n",
       "  'methylation',\n",
       "  'very',\n",
       "  'become',\n",
       "  'expressed',\n",
       "  'depending'],\n",
       " ['DNA',\n",
       "  'myelodysplastic',\n",
       "  'Decitabine',\n",
       "  'treat',\n",
       "  'class',\n",
       "  'used',\n",
       "  'belongs',\n",
       "  'agents',\n",
       "  'myelogenous',\n",
       "  'precursors',\n",
       "  'demethylating',\n",
       "  'syndromes'],\n",
       " ['effective',\n",
       "  'cancer',\n",
       "  'mitotically',\n",
       "  'having',\n",
       "  'heritable',\n",
       "  'without',\n",
       "  'may',\n",
       "  'growing',\n",
       "  'change',\n",
       "  'changes',\n",
       "  'so',\n",
       "  'might']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0,\n",
       "   '0.028*\"heritable\" + 0.025*\"mitotically\" + 0.025*\"genes\" + 0.025*\"suppressors\" + 0.024*\"spreaded\" + 0.024*\"genomic\" + 0.024*\"largely\" + 0.024*\"regions\" + 0.024*\"introns\" + 0.024*\"activation\"'),\n",
       "  (1,\n",
       "   '0.028*\"genes\" + 0.026*\"elements\" + 0.026*\"illegitimate\" + 0.026*\"genomic\" + 0.026*\"cells\" + 0.025*\"intergenic\" + 0.025*\"heritable\" + 0.025*\"translocations\" + 0.024*\"diluted\" + 0.024*\"cpg\"'),\n",
       "  (2,\n",
       "   '0.026*\"proliferating\" + 0.026*\"promoters\" + 0.026*\"cell\" + 0.025*\"largely\" + 0.025*\"islands\" + 0.025*\"mitotically\" + 0.025*\"instability\" + 0.024*\"genome\" + 0.024*\"cryptic\" + 0.024*\"contributes\"'),\n",
       "  (3,\n",
       "   '0.027*\"lead\" + 0.026*\"proliferating\" + 0.026*\"suppressors\" + 0.025*\"tumor\" + 0.025*\"islands\" + 0.025*\"recombination\" + 0.025*\"cryptic\" + 0.025*\"largely\" + 0.025*\"deletions\" + 0.025*\"dna\"'),\n",
       "  (4,\n",
       "   '0.028*\"introns\" + 0.027*\"contributing\" + 0.026*\"neighbouring\" + 0.025*\"spreaded\" + 0.025*\"repeats\" + 0.025*\"likely\" + 0.025*\"present\" + 0.025*\"regions\" + 0.025*\"insertions\" + 0.024*\"genes\"'),\n",
       "  (5,\n",
       "   '0.026*\"neighbouring\" + 0.026*\"activation\" + 0.025*\"heritable\" + 0.025*\"genome\" + 0.024*\"disruption\" + 0.024*\"suppressors\" + 0.024*\"methylation\" + 0.024*\"likely\" + 0.024*\"spreaded\" + 0.024*\"cell\"'),\n",
       "  (6,\n",
       "   '0.028*\"repetitive\" + 0.026*\"translocations\" + 0.026*\"disruption\" + 0.025*\"elements\" + 0.025*\"spreaded\" + 0.025*\"intergenic\" + 0.025*\"neighbouring\" + 0.025*\"instability\" + 0.024*\"cancer\" + 0.024*\"cell\"'),\n",
       "  (7,\n",
       "   '0.028*\"mitotically\" + 0.027*\"silencing\" + 0.025*\"cryptic\" + 0.025*\"proliferating\" + 0.025*\"cell\" + 0.025*\"intergenic\" + 0.024*\"deletions\" + 0.024*\"hypomethylated\" + 0.023*\"spreaded\" + 0.023*\"illegitimate\"'),\n",
       "  (8,\n",
       "   '0.026*\"proliferating\" + 0.026*\"genome\" + 0.026*\"deletions\" + 0.026*\"contributing\" + 0.025*\"translocations\" + 0.025*\"instability\" + 0.025*\"hypermethylated\" + 0.025*\"introns\" + 0.025*\"genes\" + 0.024*\"recombination\"'),\n",
       "  (9,\n",
       "   '0.026*\"disruption\" + 0.026*\"repetitive\" + 0.025*\"genomic\" + 0.025*\"normal\" + 0.025*\"present\" + 0.025*\"cpg\" + 0.024*\"hypomethylated\" + 0.024*\"contributes\" + 0.024*\"tumor\" + 0.024*\"contributing\"'),\n",
       "  (10,\n",
       "   '0.026*\"cpg\" + 0.026*\"heritable\" + 0.025*\"diluted\" + 0.025*\"suppressors\" + 0.025*\"activation\" + 0.025*\"contributing\" + 0.025*\"likely\" + 0.025*\"deletions\" + 0.024*\"present\" + 0.024*\"mitotically\"'),\n",
       "  (11,\n",
       "   '0.026*\"contributes\" + 0.026*\"diluted\" + 0.025*\"hypermethylated\" + 0.025*\"spreaded\" + 0.025*\"methylation\" + 0.024*\"neighbouring\" + 0.024*\"introns\" + 0.024*\"lead\" + 0.024*\"suppressors\" + 0.024*\"cryptic\"')],\n",
       " [(0,\n",
       "   '0.022*\"protein\" + 0.022*\"hand\" + 0.022*\"ctfc\" + 0.021*\"agent\" + 0.021*\"igf2\" + 0.021*\"able\" + 0.021*\"pattern\" + 0.020*\"upstream\" + 0.020*\"depending\" + 0.020*\"promoting\"'),\n",
       "  (1,\n",
       "   '0.022*\"tumour\" + 0.022*\"characteristic\" + 0.022*\"alleles\" + 0.021*\"pre\" + 0.021*\"child\" + 0.021*\"cancer\" + 0.021*\"period\" + 0.020*\"neoplastic\" + 0.020*\"double\" + 0.020*\"protein\"'),\n",
       "  (2,\n",
       "   '0.022*\"period\" + 0.022*\"promoting\" + 0.021*\"pattern\" + 0.021*\"depending\" + 0.021*\"act\" + 0.021*\"hand\" + 0.021*\"imprinting\" + 0.020*\"alleles\" + 0.020*\"paternal\" + 0.020*\"neoplastic\"'),\n",
       "  (3,\n",
       "   '0.022*\"methylated\" + 0.022*\"pattern\" + 0.022*\"expressed\" + 0.021*\"silenced\" + 0.021*\"igf2\" + 0.021*\"imprinting\" + 0.021*\"unexpressed\" + 0.021*\"depending\" + 0.020*\"ctcf\" + 0.020*\"cells\"'),\n",
       "  (4,\n",
       "   '0.022*\"pattern\" + 0.021*\"icr\" + 0.021*\"characteristic\" + 0.021*\"leading\" + 0.021*\"igf2\" + 0.020*\"pre\" + 0.020*\"wilms\" + 0.020*\"loss\" + 0.020*\"presents\" + 0.020*\"unexpressed\"'),\n",
       "  (5,\n",
       "   '0.022*\"expression\" + 0.022*\"methylation\" + 0.021*\"igf2\" + 0.021*\"silenced\" + 0.020*\"methylated\" + 0.020*\"ctcf\" + 0.020*\"leading\" + 0.020*\"loss\" + 0.020*\"allele\" + 0.020*\"wilms\"'),\n",
       "  (6,\n",
       "   '0.022*\"shown\" + 0.022*\"imprinting\" + 0.022*\"methylated\" + 0.021*\"paternal\" + 0.021*\"development\" + 0.021*\"occur\" + 0.020*\"dose\" + 0.020*\"presents\" + 0.020*\"icr\" + 0.020*\"neoplastic\"'),\n",
       "  (7,\n",
       "   '0.023*\"protein\" + 0.022*\"expressed\" + 0.021*\"double\" + 0.020*\"able\" + 0.020*\"methylated\" + 0.020*\"hand\" + 0.020*\"period\" + 0.020*\"depending\" + 0.020*\"growth\" + 0.020*\"enhancers\"'),\n",
       "  (8,\n",
       "   '0.022*\"wilms\" + 0.022*\"dose\" + 0.021*\"early\" + 0.021*\"ctcf\" + 0.021*\"depending\" + 0.021*\"methylated\" + 0.021*\"characteristic\" + 0.020*\"shown\" + 0.020*\"promoting\" + 0.020*\"normal\"'),\n",
       "  (9,\n",
       "   '0.023*\"promoting\" + 0.021*\"cluster\" + 0.021*\"kidney\" + 0.021*\"paternal\" + 0.021*\"child\" + 0.021*\"period\" + 0.021*\"insulated\" + 0.020*\"growth\" + 0.020*\"icr\" + 0.020*\"double\"'),\n",
       "  (10,\n",
       "   '0.024*\"unexpressed\" + 0.022*\"event\" + 0.022*\"h19\" + 0.021*\"leading\" + 0.021*\"early\" + 0.021*\"loss\" + 0.021*\"act\" + 0.021*\"hand\" + 0.020*\"expressed\" + 0.020*\"icr\"'),\n",
       "  (11,\n",
       "   '0.022*\"cancer\" + 0.022*\"event\" + 0.021*\"child\" + 0.021*\"pre\" + 0.020*\"pattern\" + 0.020*\"leading\" + 0.020*\"protein\" + 0.020*\"presents\" + 0.020*\"silenced\" + 0.020*\"loss\"')],\n",
       " [(0,\n",
       "   '0.059*\"leukaemia\" + 0.057*\"tumour\" + 0.057*\"suppressor\" + 0.056*\"syndromes\" + 0.055*\"decitabine\" + 0.055*\"belongs\" + 0.054*\"agents\" + 0.053*\"genes\" + 0.052*\"demethylating\" + 0.052*\"potential\"'),\n",
       "  (1,\n",
       "   '0.064*\"treat\" + 0.058*\"syndromes\" + 0.056*\"precursors\" + 0.055*\"decitabine\" + 0.055*\"dna\" + 0.054*\"agents\" + 0.054*\"belongs\" + 0.054*\"acute\" + 0.053*\"leukaemia\" + 0.053*\"myelodysplastic\"'),\n",
       "  (2,\n",
       "   '0.060*\"belongs\" + 0.059*\"myelodysplastic\" + 0.059*\"leukaemia\" + 0.058*\"decitabine\" + 0.057*\"genes\" + 0.056*\"agents\" + 0.055*\"acute\" + 0.054*\"suppressor\" + 0.054*\"demethylate\" + 0.053*\"dna\"'),\n",
       "  (3,\n",
       "   '0.064*\"agents\" + 0.064*\"precursors\" + 0.062*\"class\" + 0.057*\"potential\" + 0.055*\"genes\" + 0.055*\"dna\" + 0.053*\"demethylate\" + 0.052*\"treat\" + 0.052*\"decitabine\" + 0.052*\"demethylating\"'),\n",
       "  (4,\n",
       "   '0.060*\"example\" + 0.059*\"tumour\" + 0.058*\"class\" + 0.057*\"leukaemia\" + 0.057*\"genes\" + 0.056*\"decitabine\" + 0.055*\"demethylate\" + 0.054*\"myelodysplastic\" + 0.054*\"syndromes\" + 0.054*\"dna\"'),\n",
       "  (5,\n",
       "   '0.060*\"example\" + 0.058*\"syndromes\" + 0.058*\"tumour\" + 0.056*\"myelodysplastic\" + 0.056*\"acute\" + 0.055*\"genes\" + 0.055*\"belongs\" + 0.054*\"precursors\" + 0.054*\"treat\" + 0.054*\"potential\"'),\n",
       "  (6,\n",
       "   '0.061*\"demethylate\" + 0.059*\"treat\" + 0.059*\"precursors\" + 0.058*\"agents\" + 0.057*\"syndromes\" + 0.056*\"class\" + 0.055*\"dna\" + 0.054*\"genes\" + 0.053*\"demethylating\" + 0.052*\"potential\"'),\n",
       "  (7,\n",
       "   '0.061*\"tumour\" + 0.059*\"myelogenous\" + 0.059*\"class\" + 0.058*\"suppressor\" + 0.056*\"agents\" + 0.055*\"treat\" + 0.055*\"genes\" + 0.055*\"precursors\" + 0.054*\"leukaemia\" + 0.054*\"dna\"'),\n",
       "  (8,\n",
       "   '0.063*\"genes\" + 0.061*\"treat\" + 0.058*\"decitabine\" + 0.056*\"acute\" + 0.055*\"suppressor\" + 0.055*\"myelogenous\" + 0.055*\"demethylating\" + 0.054*\"belongs\" + 0.054*\"syndromes\" + 0.053*\"demethylate\"'),\n",
       "  (9,\n",
       "   '0.063*\"potential\" + 0.059*\"decitabine\" + 0.058*\"leukaemia\" + 0.056*\"myelogenous\" + 0.056*\"belongs\" + 0.055*\"demethylating\" + 0.054*\"treat\" + 0.053*\"suppressor\" + 0.053*\"syndromes\" + 0.052*\"example\"'),\n",
       "  (10,\n",
       "   '0.066*\"demethylating\" + 0.063*\"example\" + 0.061*\"suppressor\" + 0.060*\"genes\" + 0.059*\"potential\" + 0.056*\"class\" + 0.055*\"belongs\" + 0.054*\"demethylate\" + 0.053*\"syndromes\" + 0.053*\"treat\"'),\n",
       "  (11,\n",
       "   '0.059*\"potential\" + 0.059*\"acute\" + 0.057*\"genes\" + 0.056*\"syndromes\" + 0.056*\"dna\" + 0.056*\"precursors\" + 0.056*\"myelogenous\" + 0.055*\"class\" + 0.054*\"agents\" + 0.053*\"demethylating\"')],\n",
       " [(0,\n",
       "   '0.041*\"cancer\" + 0.040*\"development\" + 0.039*\"inadvisable\" + 0.039*\"woman\" + 0.037*\"stop\" + 0.036*\"effective\" + 0.036*\"marks\" + 0.036*\"change\" + 0.035*\"established\" + 0.035*\"embryo\"'),\n",
       "  (1,\n",
       "   '0.043*\"established\" + 0.042*\"cells\" + 0.041*\"use\" + 0.039*\"change\" + 0.039*\"heritable\" + 0.037*\"group\" + 0.037*\"growing\" + 0.036*\"changes\" + 0.036*\"kill\" + 0.036*\"woman\"'),\n",
       "  (2,\n",
       "   '0.040*\"established\" + 0.039*\"inadvisable\" + 0.039*\"example\" + 0.037*\"drugs\" + 0.037*\"effective\" + 0.037*\"use\" + 0.036*\"cells\" + 0.036*\"cancer\" + 0.036*\"mitotically\" + 0.036*\"development\"'),\n",
       "  (3,\n",
       "   '0.041*\"changes\" + 0.041*\"cells\" + 0.041*\"epigenetic\" + 0.040*\"use\" + 0.038*\"development\" + 0.038*\"inadvisable\" + 0.036*\"kill\" + 0.036*\"stop\" + 0.036*\"marks\" + 0.036*\"periods\"'),\n",
       "  (4,\n",
       "   '0.042*\"heritable\" + 0.041*\"embryo\" + 0.040*\"effective\" + 0.040*\"sensitive\" + 0.038*\"development\" + 0.037*\"marks\" + 0.037*\"change\" + 0.036*\"epigenetic\" + 0.036*\"pregnant\" + 0.036*\"use\"'),\n",
       "  (5,\n",
       "   '0.042*\"kill\" + 0.040*\"changes\" + 0.040*\"early\" + 0.039*\"remodelled\" + 0.038*\"epigenetic\" + 0.038*\"pregnant\" + 0.036*\"development\" + 0.036*\"effective\" + 0.035*\"heritable\" + 0.035*\"marks\"'),\n",
       "  (6,\n",
       "   '0.040*\"development\" + 0.039*\"stop\" + 0.037*\"early\" + 0.036*\"changes\" + 0.036*\"epigenetic\" + 0.036*\"use\" + 0.036*\"change\" + 0.036*\"established\" + 0.036*\"cells\" + 0.036*\"germ\"'),\n",
       "  (7,\n",
       "   '0.041*\"kill\" + 0.039*\"epigenetic\" + 0.038*\"pregnant\" + 0.037*\"marks\" + 0.037*\"heritable\" + 0.036*\"growing\" + 0.036*\"having\" + 0.036*\"stop\" + 0.036*\"embryo\" + 0.035*\"cancer\"'),\n",
       "  (8,\n",
       "   '0.041*\"epigenetic\" + 0.040*\"periods\" + 0.040*\"pregnant\" + 0.038*\"cancer\" + 0.036*\"example\" + 0.036*\"having\" + 0.036*\"drugs\" + 0.035*\"change\" + 0.035*\"use\" + 0.035*\"effective\"'),\n",
       "  (9,\n",
       "   '0.043*\"inadvisable\" + 0.039*\"having\" + 0.038*\"example\" + 0.038*\"heritable\" + 0.037*\"periods\" + 0.036*\"development\" + 0.036*\"kill\" + 0.036*\"sensitive\" + 0.036*\"marks\" + 0.035*\"embryo\"'),\n",
       "  (10,\n",
       "   '0.045*\"having\" + 0.044*\"established\" + 0.040*\"effective\" + 0.038*\"change\" + 0.037*\"changes\" + 0.037*\"epigenetic\" + 0.037*\"mitotically\" + 0.036*\"stop\" + 0.036*\"embryo\" + 0.035*\"example\"'),\n",
       "  (11,\n",
       "   '0.039*\"periods\" + 0.038*\"line\" + 0.038*\"woman\" + 0.038*\"development\" + 0.037*\"early\" + 0.037*\"epigenetic\" + 0.037*\"sensitive\" + 0.037*\"cancer\" + 0.036*\"stop\" + 0.036*\"cells\"')]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDAMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
